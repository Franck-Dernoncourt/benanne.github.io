* make a 'software' page:
    * morb
    * kaggle-whales
    * ...

* write first post

* extend about page

* test disqus

* test share buttons


post ideas

* Don't pretrain that neural net!
    - unsupervised pretraining is no longer necessary in the majority of cases
    - problem: vanishing gradients, bad local minima (optimization is hard)
    - relus + dropout (+ some more recent advances such as maxout, stochastic pooling)
    - relus make the optimization much easier (intuitively: because they are piecewise linear)
    - unsupervised pretraining can still be useful if labeled data is scarce (and unlabeled data isn't)
    - lately the merits of supervised pretraining have been clearly shown (think ccv, overfeat)
    - summary: in the general, unsupervised pretraining is obsolete. Just make sure to use relus and dropout.

    DWF quote: http://www.reddit.com/r/MachineLearning/comments/20wh21/how_does_regularization_affect_dropout_while/
    + he also brings up another interesting point: there are better ways to do unsupervised feature learning if that's really what you want to do. The inference procedure of RBMs and autoencoders is feed-forward, so there is very little competition between features, something which has been proven to be important (+ sparse autoencoders don't have 'real' sparsity). For pretraining feed-forward is essential, but if you're just doing feature learning this is not a requirement and there are better options. I personally like (spherical) k-means a lot because it yields good features and is several orders of magnitude faster to train than autoencoders/RBMs (ref. adam coates).

    """
    If you're going to do classification or regression [...], you ought to try a feed forward ReLU MLP trained with SGD, momentum, dropout and norm constraints before you waste your time with the conceptually ill-posed mess that is unsupervised pre-training.
    """

    Friends don't let friends pretrain neural nets!


* How to use cuda-convnet convolutions from pylearn2 in Theano
    - why not just use cuda-convnet? Because C++ is tedious, Theano has many advantages (lets you express models mathematically, lots of flexibility, rapid prototyping)
    - why not just use pylearn2? although it is aimed at researchers, it has a steep learning curve + theano offers complete flexibility (of course the flip side of the coin is that you need to write more stuff yourself)
    - poorly documented, so this post tries to piece things together
    - not a drop-in replacement for theano.tensor.nnet.conv2d / max_pool_2d
        * some limitations: square pooling regions, square filters, multiples of 16 here and there
        * some advantages: strided pooling, A LOT FASTER (speedups 2x, 3x on GTX 680 for which it was not optimised)
        * need to tune performance/memory usage with partial_sum (although 1 and None are probably the only settings you need)
    - flexibility of Theano + speed of cuda-convnet is a very powerful combination (cfr. Galaxy Zoo)

* describe the galaxy zoo network