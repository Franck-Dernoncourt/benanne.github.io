* test share buttons



post ideas

* Don't pretrain that neural net!
    - unsupervised pretraining is no longer necessary in the majority of cases
    - problem: vanishing gradients, bad local minima (optimization is hard)
    - relus + dropout (+ some more recent advances such as maxout, stochastic pooling)
    - relus make the optimization much easier (intuitively: because they are piecewise linear)
    - unsupervised pretraining can still be useful if labeled data is scarce (and unlabeled data isn't)
    - lately the merits of supervised pretraining have been clearly shown (think ccv, overfeat)
    - summary: in the general, unsupervised pretraining is obsolete. Just make sure to use relus and dropout.

    DWF quote: http://www.reddit.com/r/MachineLearning/comments/20wh21/how_does_regularization_affect_dropout_while/
    + he also brings up another interesting point: there are better ways to do unsupervised feature learning if that's really what you want to do. The inference procedure of RBMs and autoencoders is feed-forward, so there is very little competition between features, something which has been proven to be important (+ sparse autoencoders don't have 'real' sparsity). For pretraining feed-forward is essential, but if you're just doing feature learning this is not a requirement and there are better options. I personally like (spherical) k-means a lot because it yields good features and is several orders of magnitude faster to train than autoencoders/RBMs (ref. adam coates).

    """
    If you're going to do classification or regression [...], you ought to try a feed forward ReLU MLP trained with SGD, momentum, dropout and norm constraints before you waste your time with the conceptually ill-posed mess that is unsupervised pre-training.
    """

    YLC quote: https://twitter.com/t3kcit/status/451771492984823808

    Friends don't let friends pretrain neural nets!

* Random search for hyperparameter optimization vs. grid search, hyperopt, spearmint

* describe the galaxy zoo network